{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n",
    "\n",
    "Import the required libraries.\n",
    "\n",
    "    #Kaggle's reader needed these.\n",
    "    import tensorflow as tf\n",
    "    import re\n",
    "    from typing import Dict, List, Optional, Text, Tuple\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    from matplotlib import colors\n",
    "    from scipy.ndimage import generic_filter\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay\n",
    "    from sklearn.metrics import r2_score\n",
    "    import numpy as np\n",
    "    import random\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import seaborn as sns\n",
    "\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.metrics import categorical_crossentropy\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from keras.layers import Input, Lambda, Dense, Flatten\n",
    "    from keras.models import Model\n",
    "    import tensorflow.keras.applications.resnet50\n",
    "    from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    from keras.preprocessing import image\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    from keras.models import Sequential\n",
    "    import numpy as np\n",
    "    from glob import glob\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cv2\n",
    "    import imghdr\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from sklearn import datasets\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import itertools\n",
    "\n",
    "Load the dataset\n",
    "\n",
    "\\*\\*\\*\\*Acquired code (denoted by brackets \\[\\])\\*\\*\\*\\* \\[\n",
    "\n",
    "    \"\"\"Constants for the data reader.\"\"\"\n",
    "\n",
    "    INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph', \n",
    "                      'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
    "\n",
    "    OUTPUT_FEATURES = ['FireMask', ]\n",
    "\n",
    "    # Data statistics \n",
    "    # For each variable, the statistics are ordered in the form:\n",
    "    # (min_clip, max_clip, mean, standard deviation)\n",
    "    DATA_STATS = {\n",
    "        # Elevation in m.\n",
    "        # 0.1 percentile, 99.9 percentile\n",
    "        'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
    "        \n",
    "        # Drought Index (Palmer Drought Severity Index)\n",
    "        # 0.1 percentile, 99.9 percentile\n",
    "        'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n",
    "        \n",
    "        #Vegetation index (times 10,000 maybe, since it's supposed to be b/w -1 and 1?)\n",
    "        'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),  # min, max\n",
    "       \n",
    "        # Precipitation in mm.\n",
    "        # Negative values do not make sense, so min is set to 0.\n",
    "        # 0., 99.9 percentile\n",
    "        'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n",
    "       \n",
    "        # Specific humidity.\n",
    "        # Negative values do not make sense, so min is set to 0.\n",
    "        # The range of specific humidity is up to 100% so max is 1.\n",
    "        'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
    "        \n",
    "        # Wind direction in degrees clockwise from north.\n",
    "        # Thus min set to 0 and max set to 360.\n",
    "        'th': (0., 360.0, 190.32976, 72.59854),\n",
    "        \n",
    "        # Min/max temperature in Kelvin.\n",
    "        \n",
    "        #Min temp\n",
    "        # -20 degree C, 99.9 percentile\n",
    "        'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n",
    "        \n",
    "        #Max temp\n",
    "        # -20 degree C, 99.9 percentile\n",
    "        'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n",
    "        \n",
    "        # Wind speed in m/s.\n",
    "        # Negative values do not make sense, given there is a wind direction.\n",
    "        # 0., 99.9 percentile\n",
    "        'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n",
    "        \n",
    "        # NFDRS fire danger index energy release component expressed in BTU's per\n",
    "        # square foot.\n",
    "        # Negative values do not make sense. Thus min set to zero.\n",
    "        # 0., 99.9 percentile\n",
    "        'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n",
    "        \n",
    "        # Population density\n",
    "        # min, 99.9 percentile\n",
    "        'population': (0., 2534.06298828125, 25.531384, 154.72331),\n",
    "        \n",
    "        # We don't want to normalize the FireMasks.\n",
    "        # 1 indicates fire, 0 no fire, -1 unlabeled data\n",
    "        'PrevFireMask': (-1., 1., 0., 1.),\n",
    "        'FireMask': (-1., 1., 0., 1.)\n",
    "    }\n",
    "\n",
    "The following cell defines cropping functions for extracting regions of\n",
    "the desired size from the input data.\n",
    "\n",
    "    \"\"\"Library of common functions used in deep learning neural networks.\n",
    "    \"\"\"\n",
    "    #YOU PROBABLY WILL NOT USE THESE.\n",
    "\n",
    "    def random_crop_input_and_output_images(\n",
    "        input_img: tf.Tensor,\n",
    "        output_img: tf.Tensor,\n",
    "        sample_size: int,\n",
    "        num_in_channels: int,\n",
    "        num_out_channels: int,\n",
    "    ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "      \"\"\"Randomly axis-align crop input and output image tensors.\n",
    "\n",
    "      Args:\n",
    "        input_img: tensor with dimensions HWC.\n",
    "        output_img: tensor with dimensions HWC.\n",
    "        sample_size: side length (square) to crop to.\n",
    "        num_in_channels: number of channels in input_img.\n",
    "        num_out_channels: number of channels in output_img.\n",
    "      Returns:\n",
    "        input_img: tensor with dimensions HWC.\n",
    "        output_img: tensor with dimensions HWC.\n",
    "      \"\"\"\n",
    "      combined = tf.concat([input_img, output_img], axis=2)\n",
    "      combined = tf.image.random_crop(\n",
    "          combined,\n",
    "          [sample_size, sample_size, num_in_channels + num_out_channels])\n",
    "      input_img = combined[:, :, 0:num_in_channels]\n",
    "      output_img = combined[:, :, -num_out_channels:]\n",
    "      return input_img, output_img\n",
    "\n",
    "\n",
    "    def center_crop_input_and_output_images(\n",
    "        input_img: tf.Tensor,\n",
    "        output_img: tf.Tensor,\n",
    "        sample_size: int,\n",
    "    ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "      \"\"\"Center crops input and output image tensors.\n",
    "\n",
    "      Args:\n",
    "        input_img: tensor with dimensions HWC.\n",
    "        output_img: tensor with dimensions HWC.\n",
    "        sample_size: side length (square) to crop to.\n",
    "      Returns:\n",
    "        input_img: tensor with dimensions HWC.\n",
    "        output_img: tensor with dimensions HWC.\n",
    "      \"\"\"\n",
    "      central_fraction = sample_size / input_img.shape[0]\n",
    "      input_img = tf.image.central_crop(input_img, central_fraction)\n",
    "      output_img = tf.image.central_crop(output_img, central_fraction)\n",
    "      return input_img, output_img\n",
    "\n",
    "The following cell provides code for parsing the contents of the\n",
    "TensorFlow Record files. In addition to loading the data, it also offers\n",
    "functions for various preprocessing operations, such as clipping,\n",
    "rescaling, or normalizing the data.\n",
    "\n",
    "    \"\"\"Dataset reader for Earth Engine data.\"\"\"\n",
    "\n",
    "    def _get_base_key(key: Text) -> Text:\n",
    "      \"\"\"Extracts the base key from the provided key.\n",
    "\n",
    "      Earth Engine exports TFRecords containing each data variable with its\n",
    "      corresponding variable name. In the case of time sequences, the name of the\n",
    "      data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n",
    "      where 'variable' is the name of the variable, and n the number of elements\n",
    "      in the time sequence. Extracting the base key ensures that each step of the\n",
    "      time sequence goes through the same normalization steps.\n",
    "      The base key obeys the following naming pattern: '([a-zA-Z]+)'\n",
    "      For instance, for an input key 'variable_1', this function returns 'variable'.\n",
    "      For an input key 'variable', this function simply returns 'variable'.\n",
    "\n",
    "      Args:\n",
    "        key: Input key.\n",
    "\n",
    "      Returns:\n",
    "        The corresponding base key.\n",
    "\n",
    "      Raises:\n",
    "        ValueError when `key` does not match the expected pattern.\n",
    "      \"\"\"\n",
    "      match = re.match(r'([a-zA-Z]+)', key)\n",
    "      if match:\n",
    "        return match.group(1)\n",
    "      raise ValueError(\n",
    "          'The provided key does not match the expected pattern: {}'.format(key))\n",
    "\n",
    "\n",
    "    def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
    "      \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n",
    "\n",
    "      Args:\n",
    "        inputs: Inputs to clip and rescale.\n",
    "        key: Key describing the inputs.\n",
    "\n",
    "      Returns:\n",
    "        Clipped and rescaled input.\n",
    "\n",
    "      Raises:\n",
    "        ValueError if there are no data statistics available for `key`.\n",
    "      \"\"\"\n",
    "      base_key = _get_base_key(key)\n",
    "      if base_key not in DATA_STATS:\n",
    "        raise ValueError(\n",
    "            'No data statistics available for the requested key: {}.'.format(key))\n",
    "      min_val, max_val, _, _ = DATA_STATS[base_key]\n",
    "      inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
    "      return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n",
    "\n",
    "\n",
    "    def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
    "      \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n",
    "\n",
    "      Args:\n",
    "        inputs: Inputs to clip and normalize.\n",
    "        key: Key describing the inputs.\n",
    "\n",
    "      Returns:\n",
    "        Clipped and normalized input.\n",
    "\n",
    "      Raises:\n",
    "        ValueError if there are no data statistics available for `key`.\n",
    "      \"\"\"\n",
    "      base_key = _get_base_key(key)\n",
    "      if base_key not in DATA_STATS:\n",
    "        raise ValueError(\n",
    "            'No data statistics available for the requested key: {}.'.format(key))\n",
    "      min_val, max_val, mean, std = DATA_STATS[base_key]\n",
    "      inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
    "      inputs = inputs - mean\n",
    "      return tf.math.divide_no_nan(inputs, std)\n",
    "\n",
    "    def _get_features_dict(\n",
    "        sample_size: int,\n",
    "        features: List[Text],\n",
    "    ) -> Dict[Text, tf.io.FixedLenFeature]:\n",
    "      \"\"\"Creates a features dictionary for TensorFlow IO.\n",
    "\n",
    "      Args:\n",
    "        sample_size: Size of the input tiles (square).\n",
    "        features: List of feature names.\n",
    "\n",
    "      Returns:\n",
    "        A features dictionary for TensorFlow IO.\n",
    "      \"\"\"\n",
    "      sample_shape = [sample_size, sample_size]\n",
    "      features = set(features)\n",
    "      columns = [\n",
    "          tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
    "          for _ in features\n",
    "      ]\n",
    "      return dict(zip(features, columns))\n",
    "\n",
    "\n",
    "    def _parse_fn(\n",
    "        example_proto: tf.train.Example, data_size: int, sample_size: int,\n",
    "        num_in_channels: int, clip_and_normalize: bool,\n",
    "        clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n",
    "    ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "      \"\"\"Reads a serialized example.\n",
    "\n",
    "      Args:\n",
    "        example_proto: A TensorFlow example protobuf.\n",
    "        data_size: Size of tiles (square) as read from input files.\n",
    "        sample_size: Size the tiles (square) when input into the model.\n",
    "        num_in_channels: Number of input channels.\n",
    "        clip_and_normalize: True if the data should be clipped and normalized.\n",
    "        clip_and_rescale: True if the data should be clipped and rescaled.\n",
    "        random_crop: True if the data should be randomly cropped.\n",
    "        center_crop: True if the data should be cropped in the center.\n",
    "\n",
    "      Returns:\n",
    "        (input_img, output_img) tuple of inputs and outputs to the ML model.\n",
    "      \"\"\"\n",
    "      if (random_crop and center_crop):\n",
    "        raise ValueError('Cannot have both random_crop and center_crop be True')\n",
    "      input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
    "      feature_names = input_features + output_features\n",
    "      features_dict = _get_features_dict(data_size, feature_names)\n",
    "      features = tf.io.parse_single_example(example_proto, features_dict)\n",
    "\n",
    "      if clip_and_normalize:\n",
    "        inputs_list = [\n",
    "            _clip_and_normalize(features.get(key), key) for key in input_features\n",
    "        ]\n",
    "      elif clip_and_rescale:\n",
    "        inputs_list = [\n",
    "            _clip_and_rescale(features.get(key), key) for key in input_features\n",
    "        ]\n",
    "      else:\n",
    "        inputs_list = [features.get(key) for key in input_features]\n",
    "      \n",
    "      inputs_stacked = tf.stack(inputs_list, axis=0)\n",
    "      input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
    "\n",
    "      outputs_list = [features.get(key) for key in output_features]\n",
    "      assert outputs_list, 'outputs_list should not be empty'\n",
    "      outputs_stacked = tf.stack(outputs_list, axis=0)\n",
    "\n",
    "      outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
    "      assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n",
    "                                                'but dimensions of outputs_stacked'\n",
    "                                                f' are {outputs_stacked_shape}')\n",
    "      output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
    "\n",
    "      if random_crop:\n",
    "        input_img, output_img = random_crop_input_and_output_images(\n",
    "            input_img, output_img, sample_size, num_in_channels, 1)\n",
    "      if center_crop:\n",
    "        input_img, output_img = center_crop_input_and_output_images(\n",
    "            input_img, output_img, sample_size)\n",
    "      return input_img, output_img\n",
    "\n",
    "\n",
    "    def get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n",
    "                    batch_size: int, num_in_channels: int, compression_type: Text,\n",
    "                    clip_and_normalize: bool, clip_and_rescale: bool,\n",
    "                    random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
    "      \"\"\"Gets the dataset from the file pattern.\n",
    "\n",
    "      Args:\n",
    "        file_pattern: Input file pattern.\n",
    "        data_size: Size of tiles (square) as read from input files.\n",
    "        sample_size: Size the tiles (square) when input into the model.\n",
    "        batch_size: Batch size.\n",
    "        num_in_channels: Number of input channels.\n",
    "        compression_type: Type of compression used for the input files.\n",
    "        clip_and_normalize: True if the data should be clipped and normalized, False\n",
    "          otherwise.\n",
    "        clip_and_rescale: True if the data should be clipped and rescaled, False\n",
    "          otherwise.\n",
    "        random_crop: True if the data should be randomly cropped.\n",
    "        center_crop: True if the data shoulde be cropped in the center.\n",
    "\n",
    "      Returns:\n",
    "        A TensorFlow dataset loaded from the input file pattern, with features\n",
    "        described in the constants, and with the shapes determined from the input\n",
    "        parameters to this function.\n",
    "      \"\"\"\n",
    "      if (clip_and_normalize and clip_and_rescale):\n",
    "        raise ValueError('Cannot have both normalize and rescale.')\n",
    "      dataset = tf.data.Dataset.list_files(file_pattern)\n",
    "      dataset = dataset.interleave(\n",
    "          lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "      dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "      dataset = dataset.map(\n",
    "          lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
    "              x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
    "              clip_and_rescale, random_crop, center_crop),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "      dataset = dataset.batch(batch_size)\n",
    "      dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "      return dataset\n",
    "\n",
    "\\]\n",
    "\n",
    "Load the dataset.\n",
    "\n",
    "The data are stored as 64x64 km regions. For each data sample, we\n",
    "extract a random 32x32 km region. In the following function call, we do\n",
    "not clip, rescale or normalize the data.\n",
    "\n",
    "TF Datasets are loaded lazily, so materialize the first batch of inputs\n",
    "and labels.\n",
    "\n",
    "    side_length = 64 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
    "    num_obs = 100 #batch size\n",
    "\n",
    "    dataset = get_dataset(\n",
    "          file_pattern,\n",
    "          data_size=64,\n",
    "          sample_size=side_length,\n",
    "          batch_size=num_obs,\n",
    "          num_in_channels=12,\n",
    "          compression_type=None,\n",
    "          clip_and_normalize=False,\n",
    "          clip_and_rescale=False,\n",
    "          random_crop=True,\n",
    "          center_crop=False)\n",
    "\n",
    "\n",
    "    User settings:\n",
    "\n",
    "       KMP_AFFINITY=granularity=fine,verbose,compact,1,0\n",
    "       KMP_BLOCKTIME=0\n",
    "       KMP_DUPLICATE_LIB_OK=True\n",
    "       KMP_INIT_AT_FORK=FALSE\n",
    "       KMP_SETTINGS=1\n",
    "       KMP_WARNINGS=0\n",
    "\n",
    "    Effective settings:\n",
    "\n",
    "       KMP_ABORT_DELAY=0\n",
    "       KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
    "       KMP_ALIGN_ALLOC=64\n",
    "       KMP_ALL_THREADPRIVATE=128\n",
    "       KMP_ATOMIC_MODE=2\n",
    "       KMP_BLOCKTIME=0\n",
    "       KMP_CPUINFO_FILE: value is not defined\n",
    "       KMP_DETERMINISTIC_REDUCTION=false\n",
    "       KMP_DEVICE_THREAD_LIMIT=2147483647\n",
    "       KMP_DISP_NUM_BUFFERS=7\n",
    "       KMP_DUPLICATE_LIB_OK=true\n",
    "       KMP_ENABLE_TASK_THROTTLING=true\n",
    "       KMP_FORCE_REDUCTION: value is not defined\n",
    "       KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
    "       KMP_FORKJOIN_BARRIER='2,2'\n",
    "       KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
    "       KMP_GTID_MODE=3\n",
    "       KMP_HANDLE_SIGNALS=false\n",
    "       KMP_HOT_TEAMS_MAX_LEVEL=1\n",
    "       KMP_HOT_TEAMS_MODE=0\n",
    "       KMP_INIT_AT_FORK=true\n",
    "       KMP_LIBRARY=throughput\n",
    "       KMP_LOCK_KIND=queuing\n",
    "       KMP_MALLOC_POOL_INCR=1M\n",
    "       KMP_NUM_LOCKS_IN_BLOCK=1\n",
    "       KMP_PLAIN_BARRIER='2,2'\n",
    "       KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
    "       KMP_REDUCTION_BARRIER='1,1'\n",
    "       KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
    "       KMP_SCHEDULE='static,balanced;guided,iterative'\n",
    "       KMP_SETTINGS=true\n",
    "       KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
    "       KMP_STACKOFFSET=64\n",
    "       KMP_STACKPAD=0\n",
    "       KMP_STACKSIZE=8M\n",
    "       KMP_STORAGE_MAP=false\n",
    "       KMP_TASKING=2\n",
    "       KMP_TASKLOOP_MIN_TASKS=0\n",
    "       KMP_TASK_STEALING_CONSTRAINT=1\n",
    "       KMP_TEAMS_THREAD_LIMIT=4\n",
    "       KMP_TOPOLOGY_METHOD=all\n",
    "       KMP_USE_YIELD=1\n",
    "       KMP_VERSION=false\n",
    "       KMP_WARNINGS=false\n",
    "       OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
    "       OMP_ALLOCATOR=omp_default_mem_alloc\n",
    "       OMP_CANCELLATION=false\n",
    "       OMP_DEFAULT_DEVICE=0\n",
    "       OMP_DISPLAY_AFFINITY=false\n",
    "       OMP_DISPLAY_ENV=false\n",
    "       OMP_DYNAMIC=false\n",
    "       OMP_MAX_ACTIVE_LEVELS=1\n",
    "       OMP_MAX_TASK_PRIORITY=0\n",
    "       OMP_NESTED: deprecated; max-active-levels-var=1\n",
    "       OMP_NUM_THREADS: value is not defined\n",
    "       OMP_PLACES: value is not defined\n",
    "       OMP_PROC_BIND='intel'\n",
    "       OMP_SCHEDULE='static'\n",
    "       OMP_STACKSIZE=8M\n",
    "       OMP_TARGET_OFFLOAD=DEFAULT\n",
    "       OMP_THREAD_LIMIT=2147483647\n",
    "       OMP_WAIT_POLICY=PASSIVE\n",
    "       KMP_AFFINITY='verbose,warnings,respect,granularity=fine,compact,1,0'\n",
    "\n",
    "    inputs, labels = next(iter(dataset)) \n",
    "    #Are there two assignments happening on every iteration because dataset stores inputs with labels?\n",
    "    #print(inputs.shape) #(100, 32, 32, 12)\n",
    "    #print(labels.shape) #(100, 32, 32, 1)\n",
    "    #print(inputs[0, :, :, 11]) #Trying to grab the previous fire mask. (Apparent) success!\n",
    "    #print(labels[0,:, :, 0]) #Ok, I think the labels are the fire mask. (That also accords with standard usage of the term.)\n",
    "\n",
    "\\*\\*\\*\\*Acquired code\\*\\*\\*\\*\n",
    "\n",
    "\\[\n",
    "\n",
    "Functions\n",
    "\n",
    "    #Function to give me a neighborhood average for a specified neighbor size. Tested and appears to work!\n",
    "    def nbr_avg(arr_in, city_radius):\n",
    "        #city_radius is how many \"layers\" out you want to go from the central pixel while calculating the avg.\n",
    "        \n",
    "        footprint = np.ones((2*city_radius+1, 2*city_radius+1))\n",
    "        footprint[city_radius, city_radius] = 0 #don't want to include the central pixel in the averaging\n",
    "        \n",
    "        arr_out = generic_filter(arr_in.astype('float'), np.nanmean, footprint=footprint, mode = 'constant', cval = np.nan)\n",
    "        \n",
    "        return arr_out\n",
    "\n",
    "    '''\n",
    "    #Source: https://gis.stackexchange.com/questions/254753/calculate-the-average-of-neighbor-pixels-for-raster-edge\n",
    "    arr_out = generic_filter(arr_in.astype('float'), np.nanmean, footprint=footprint,\n",
    "            mode='constant', cval=np.nan)\n",
    "\n",
    "    ^^^ Source gave this line.\n",
    "\n",
    "    Enlightenment has happened! Setting cval = nan (and mode = 'constant') means you're padding the array with nan, and then \n",
    "    setting the function nanmean lets you ignore that padding when you take the mean! Just like you wanted! \n",
    "    '''\n",
    "\n",
    "    \"\\n#Source: https://gis.stackexchange.com/questions/254753/calculate-the-average-of-neighbor-pixels-for-raster-edge\\narr_out = generic_filter(arr_in.astype('float'), np.nanmean, footprint=footprint,\\n        mode='constant', cval=np.nan)\\n\\n^^^ Source gave this line.\\n\\nEnlightenment has happened! Setting cval = nan (and mode = 'constant') means you're padding the array with nan, and then \\nsetting the function nanmean lets you ignore that padding when you take the mean! Just like you wanted! \\n\"\n",
    "\n",
    "    def avg_neighbor_batch(batch_in, city_radius):\n",
    "        batch_size, rows, cols = batch_in.shape \n",
    "        batch_out = np.zeros((batch_size, rows, cols))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            working_arr = batch_in[i,:,:]\n",
    "            avgd_arr = nbr_avg(working_arr, city_radius=city_radius)\n",
    "            batch_out[i,:,:] =  avgd_arr\n",
    "        #/for loop\n",
    "        \n",
    "        return batch_out\n",
    "\n",
    "    #Function to cull (input, label) fire mask pairs where there is missing data in either. \n",
    "    #Function input: a batch from the data reader\n",
    "    #Function output: only those observations (grids) from the batch with certain data. Also now the format is np array not tensor.\n",
    "\n",
    "    def cull(reader_batch, reader_labels):\n",
    "        reader_batch = np.array(reader_batch) \n",
    "        prev_masks = np.squeeze(reader_batch[:,:,:,11])\n",
    "        now_masks = np.squeeze(np.array(reader_labels))\n",
    "        \n",
    "        num_imgs, rows, cols = prev_masks.shape\n",
    "        \n",
    "        #find uncertain data \n",
    "        \n",
    "        count = 0 #included for testing only\n",
    "        indices = []\n",
    "\n",
    "        for img_num in range(num_imgs): \n",
    "            working_prev_mask = prev_masks[img_num, :, :] #grab the \"working fire mask\" off the pile\n",
    "            working_now_mask = now_masks[img_num,:,:]\n",
    "            \n",
    "            if (np.all( np.invert(working_prev_mask == -1) ) and np.all( np.invert(working_now_mask == -1))): \n",
    "            #If no missing data, condition is TRUE.\n",
    "                count += 1\n",
    "                indices.append(img_num)\n",
    "                \n",
    "                #\\if statement\n",
    "           \n",
    "            #\\for loop\n",
    "        \n",
    "        culled_batch = reader_batch[indices,:,:,:]\n",
    "        culled_labels = now_masks[indices]\n",
    "        \n",
    "        return culled_batch, culled_labels\n",
    "\n",
    "    #Function that computes sum{ min( 0, (d-hat dot w-hat)*f ) } where: \n",
    "    #w-hat is the wind direction unit vector times the wind speed,\n",
    "    #d-hat is the unit vector from a neighboring pixel to the pixel of interest, \n",
    "    #f is 1 if there is fire in the neighboring square and 0 otherwise,\n",
    "    #and the sum is taken over all neighboring pixels.\n",
    "\n",
    "    #Function input: culled_inputs (numpy array)\n",
    "    #Function output: 3D numpy array holding the firewind scores at each pixel (rows, cols) in each observation (sheets)\n",
    "\n",
    "    def calc_firewind_scores(culled_inputs):\n",
    "    #Realized that you really want the wind speed to be the wind speed at the neighboring pixel. Otherwise sum_{d-hat} w dot d-hat \n",
    "    #is going to be identically 0! Oops. \n",
    "        \n",
    "        #First, an inner function that I'll want later\n",
    "        def firewind_contribution(nbr_fire, nbr_dir_vec, wind_dir_vec):\n",
    "            contribution = min( 0, nbr_fire*(np.dot(nbr_dir_vec, wind_dir_vec)) )\n",
    "            #Why setting the min at 0: If you're on fire but the wind is blowing away from you, I didn't think \n",
    "            #that would help put out the fire at your location. So you shouldn't get negative contributions from wind.\n",
    "            \n",
    "            return contribution\n",
    "        \n",
    "            #/inner function\n",
    "            \n",
    "            \n",
    "        num_imgs, rows, cols, feats = culled_inputs.shape\n",
    "        firewind_scores = np.zeros((num_imgs, rows, cols))\n",
    "        \n",
    "        for img_num in range(num_imgs):\n",
    "            wind_speed_array = culled_inputs[img_num,:,:,2]\n",
    "            wind_angle_array = culled_inputs[img_num,:,:,1]\n",
    "            fire_or_not = culled_inputs[img_num,:,:,11]\n",
    "            for i in range(rows):\n",
    "                for j in range(cols):\n",
    "                    firewind_score = 0\n",
    "                    \n",
    "                    #Now I need to add up the contributions to the windfire scores from each neighboring pixel:\n",
    "                    #And remember, the input order is: firewind_contribution(nbr_fire, nbr_dir_vec, wind_dir_vec):\n",
    "                    if (i != 0): #If we're not on the top row\n",
    "                        #try adding the contributions from neighbors in the row above\n",
    "                        if (j != 0): #If we're not on the left edge\n",
    "                            #add the contribution from the NW neighbor\n",
    "                            nbr_fire = fire_or_not[i-1,j-1]\n",
    "                            wind_speed = wind_speed_array[i-1,j-1]\n",
    "                            wind_dir = wind_angle_array[i-1, j-1]\n",
    "                            wind_dir_vec = np.array([-np.cos(np.deg2rad(wind_dir)), -np.sin(np.deg2rad(wind_dir))])\n",
    "                            wind_vec = wind_speed * wind_dir_vec\n",
    "                            nbr_dir_vec = [np.cos(-np.pi/4), np.sin(-np.pi/4)]\n",
    "                            firewind_score += firewind_contribution(nbr_fire, nbr_dir_vec, wind_dir_vec)\n",
    "                        \n",
    "                        #One way of another we're adding the neighbor directly above\n",
    "                        nbr_fire = fire_or_not[i-1,j]\n",
    "                        wind_speed = wind_speed_array[i-1,j]\n",
    "                        wind_dir = wind_angle_array[i-1, j]\n",
    "                        wind_dir_vec = np.array([-np.cos(np.deg2rad(wind_dir)), -np.sin(np.deg2rad(wind_dir))])\n",
    "                        wind_vec = wind_speed * wind_dir_vec\n",
    "                        nbr_dir_vec = [0, -1]\n",
    "                        firewind_score += firewind_contribution(nbr_fire, nbr_dir_vec, wind_dir_vec)\n",
    "                        \n",
    "                        if (j != (cols-1)): #If we're not on the right edge\n",
    "                            #add the contribution from the NE neighbor\n",
    "                            nbr_fire = fire_or_not[i-1,j+1] \n",
    "                            wind_speed = wind_speed_array[i-1,j+1]\n",
    "                            wind_dir = wind_angle_array[i-1, j+1]\n",
    "                            wind_dir_vec = np.array([-np.cos(np.deg2rad(wind_dir)), -np.sin(np.deg2rad(wind_dir))])\n",
    "                            wind_vec = wind_speed * wind_dir_vec\n",
    "                            nbr_dir_vec = [np.cos(5*np.pi/4), np.sin(5*np.pi/4)]\n",
    "                            firewind_score += firewind_contribution(nbr_fire, nbr_dir_vec, wind_dir_vec)\n",
    "                    \n",
    "                    if (i != (rows-1)): #If we're not on the bottom row\n",
    "                        #try adding the contributions from neighbors in the row below\n",
    "                        if (j != 0): #if we're not on the left edge\n",
    "                            #add the contribution from the SW neighbor\n",
    "                            nbr_fire = fire_or_not[i+1, j-1]\n",
    "                            wind_speed = wind_speed_array[i+1,j-1]\n",
    "                            wind_dir = wind_angle_array[i+1, j-1]\n",
    "                            wind_dir_vec = np.array([-np.cos(np.deg2rad(wind_dir)), -np.sin(np.deg2rad(wind_dir))])\n",
    "                            wind_vec = wind_speed * wind_dir_vec\n",
    "                            nbr_dir_vec = [np.cos(np.pi/4), np.sin(np.pi/4)]\n",
    "                            firewind_score += firewind_contribution(nbr_fire, nbr_dir_vec, wind_dir_vec)\n",
    "                        \n",
    "                        if (j != (cols-1)): #If we're not on the right edge\n",
    "                            #add the contribution from the SE neighbor\n",
    "                            nbr_fire = fire_or_not[i+1, j+1]\n",
    "                            wind_speed = wind_speed_array[i+1,j+1]\n",
    "                            wind_dir = wind_angle_array[i+1, j+1]\n",
    "                            wind_dir_vec = np.array([-np.cos(np.deg2rad(wind_dir)), -np.sin(np.deg2rad(wind_dir))])\n",
    "                            wind_vec = wind_speed * wind_dir_vec\n",
    "                            nbr_dir_vec = [np.cos(3*np.pi/4), np.sin(3*np.pi/4)]\n",
    "                            firewind_score += firewind_contribution(nbr_fire, nbr_dir_vec, wind_dir_vec)\n",
    "                            \n",
    "                        #One way or another we need the contribution from the S neighbor.\n",
    "                        nbr_fire = fire_or_not[i+1, j]\n",
    "                        wind_speed = wind_speed_array[i+1,j]\n",
    "                        wind_dir = wind_angle_array[i+1, j]\n",
    "                        wind_dir_vec = np.array([-np.cos(np.deg2rad(wind_dir)), -np.sin(np.deg2rad(wind_dir))])\n",
    "                        wind_vec = wind_speed * wind_dir_vec\n",
    "                        nbr_dir_vec = [0, 1]\n",
    "                        firewind_score += firewind_contribution(nbr_fire, nbr_dir_vec, wind_dir_vec)\n",
    "                    \n",
    "                    if (j != 0): #if we're not on the left edge\n",
    "                        #add the contribution from the W neighbor\n",
    "                        nbr_fire = fire_or_not[i, j-1]\n",
    "                        wind_speed = wind_speed_array[i,j-1]\n",
    "                        wind_dir = wind_angle_array[i, j-1]\n",
    "                        wind_dir_vec = np.array([-np.cos(np.deg2rad(wind_dir)), -np.sin(np.deg2rad(wind_dir))])\n",
    "                        wind_vec = wind_speed * wind_dir_vec\n",
    "                        nbr_dir_vec = [1, 0]\n",
    "                        firewind_score += firewind_contribution(nbr_fire, nbr_dir_vec, wind_dir_vec)\n",
    "                    \n",
    "                    if (j != (cols-1)): #if we're not on the right edge\n",
    "                        #add the contribution from the E neighbor\n",
    "                        nbr_fire = fire_or_not[i, j+1]\n",
    "                        wind_speed = wind_speed_array[i,j+1]\n",
    "                        wind_dir = wind_angle_array[i, j+1]\n",
    "                        wind_dir_vec = np.array([-np.cos(np.deg2rad(wind_dir)), -np.sin(np.deg2rad(wind_dir))])\n",
    "                        wind_vec = wind_speed * wind_dir_vec\n",
    "                        nbr_dir_vec = [-1, 0]\n",
    "                        firewind_score += firewind_contribution(nbr_fire, nbr_dir_vec, wind_dir_vec)\n",
    "                        \n",
    "                    firewind_scores[img_num, i, j] = firewind_score\n",
    "        \n",
    "        return firewind_scores\n",
    "\n",
    "    #Function to add the engineered features to the culled data. \n",
    "    #Inputs: \n",
    "        #4D (culled batch size x num rows/grid x num cols/grid x number of supplied features) np array of culled features \n",
    "        #radius of neighborhood to be considered for the \"avg nbrs on fire\" score\n",
    "    #Outputs: augmented culled_inputs array that includes the new features\n",
    "\n",
    "    def add_engineered(culled_inputs, nbrhood_radius):\n",
    "        avg_nbrs_3D_array = avg_neighbor_batch(culled_inputs[:,:,:,11], nbrhood_radius) #avg_neighbor_batch takes fire masks only \n",
    "        firewind_3D_array = calc_firewind_scores(culled_inputs)\n",
    "        to_stack = np.concatenate((avg_nbrs_3D_array[...,np.newaxis], firewind_3D_array[...,np.newaxis]), axis=3)\n",
    "        augd_culled_inputs = np.concatenate((culled_inputs,to_stack), axis=3)\n",
    "        \n",
    "        return augd_culled_inputs\n",
    "\n",
    "    #Function to vectorize data from batches of observations in a multi-D numpy array. Returns a NUMPY ARRAY.\n",
    "    #Learned on HW3 that you can use numpy.ravel() to get the same effect...\n",
    "\n",
    "    def vectorize(batched_data):\n",
    "        num_obs, rows, cols = batched_data.shape\n",
    "        #print(batched_data.shape) #used for testing\n",
    "        flat_list = []\n",
    "        for obs in range(num_obs):\n",
    "            for row in range(rows):\n",
    "                for col in range(cols):\n",
    "                    flat_list.append(batched_data[obs, row, col])\n",
    "        \n",
    "        flat_arr = np.array(flat_list)\n",
    "        return flat_arr \n",
    "\n",
    "    #Function to make 4D (img_num x row x col x feature) training matrix to pixel-based (rows are observations, cols features) one.\n",
    "    #Inputs: augmented culled inputs (so you should call add_engineered first), culled_labels\n",
    "    #Outputs: \n",
    "        #X (should be validation or test b/c we are NOT class-balancing): \n",
    "            #rows are pixels, columns are features\n",
    "        #Y: the labels. Just a vector. Index corresponds to row of the observation in the X matrix.\n",
    "\n",
    "    def grids_to_pixels(augd_culled_inputs, culled_labels):\n",
    "        \n",
    "        flat_elevation = vectorize(augd_culled_inputs[:,:,:,0])\n",
    "        #flat_wind_dir = vectorize(augd_culled_inputs[:,:,:,1]) #never used this feature other than to get the firewind score\n",
    "        flat_wind_speed = vectorize(augd_culled_inputs[:,:,:,2])\n",
    "        flat_min_temp = vectorize(augd_culled_inputs[:,:,:,3])\n",
    "        flat_max_temp = vectorize(augd_culled_inputs[:,:,:,4])\n",
    "        flat_humidity = vectorize(augd_culled_inputs[:,:,:,5])\n",
    "        flat_precipitation = vectorize(augd_culled_inputs[:,:,:,6])\n",
    "        flat_drought_index = vectorize(augd_culled_inputs[:,:,:,7])\n",
    "        flat_vegetation = vectorize(augd_culled_inputs[:,:,:,8])\n",
    "        flat_population_density = vectorize(augd_culled_inputs[:,:,:,9])\n",
    "        flat_ERC = vectorize(augd_culled_inputs[:,:,:,10])\n",
    "        flat_prev_masks = vectorize(augd_culled_inputs[:,:,:,11])\n",
    "        flat_avg_nbrs = vectorize(augd_culled_inputs[:,:,:,12])\n",
    "        flat_firewind_scores = vectorize(augd_culled_inputs[:,:,:,13])\n",
    "        \n",
    "        #Now build the flattened (rows are pixels) features array:\n",
    "        X_pix = np.array([flat_elevation, flat_wind_speed, flat_min_temp, flat_max_temp, flat_humidity, flat_precipitation, \\\n",
    "                       flat_drought_index, flat_vegetation, flat_population_density, flat_ERC, flat_prev_masks, flat_avg_nbrs, \\\n",
    "                       flat_firewind_scores]) #everything but wind direction by itself\n",
    "        X_pix = np.transpose(X_pix) #Now every row is an observation (pixel)\n",
    "        \n",
    "        #Simpler to build the flattened labels:\n",
    "        Y_pix = vectorize(culled_labels)\n",
    "        \n",
    "        return X_pix, Y_pix\n",
    "\n",
    "    #Function to use undersampling to extract two balanced classes of observations (pixels) from augd_culled_inputs.\n",
    "    #Inputs: augmented culled inputs (so you should call add_engineered first), culled_labels\n",
    "    #Outputs: \n",
    "        #X (should always training data b/c validation or test should be representative of the real-world data): \n",
    "            #a SINGLE training array where the first half (of rows) will be on fire, second half won't be\n",
    "            #rows are pixels, columns are features\n",
    "        #Y: the labels. Just a vector. Index corresponds to row of the observation in the X matrix.\n",
    "    #Also, a print statement: reports on original class sizes and culled class sizes.\n",
    "\n",
    "    def enact_undersample(augd_culled_inputs, culled_labels):\n",
    "        #TRANSITION FROM GRIDS TO PIXELS   \n",
    "        X, flat_labels = grids_to_pixels(augd_culled_inputs, culled_labels)\n",
    "        \n",
    "        #IDENTIFY THE CLASS LABELS AND INDICES OF THE OBSERVATIONS YOU'LL USE\n",
    "        indices_for_fire = [i for i in range(len(flat_labels)) if flat_labels[i] == 1]\n",
    "        indices_for_non_fire = [i for i in range(len(flat_labels)) if flat_labels[i] == 0]\n",
    "        #Quick error check:\n",
    "        if (len(indices_for_fire) > len(indices_for_non_fire)):\n",
    "            raise Exception('Are you really, really sure that the majority of your pixels are on fire?')\n",
    "        #Get the list of non-fire indices you'll actually use in the undersampling:\n",
    "        undersampled_non_fire_indices = indices_for_non_fire[0: len(indices_for_fire)] #not an off-by-one error; interval is [)\n",
    "            \n",
    "        #BUILD THE NEW X AND Y \n",
    "        fire_obs = np.take(X, indices_for_fire, axis=0) #b/c the row (axis 0) corresponds to the observation index\n",
    "        non_fire_obs = np.take(X, undersampled_non_fire_indices, axis=0)\n",
    "        \n",
    "        X_out = np.concatenate((fire_obs, non_fire_obs), axis=0)\n",
    "        Y_out = np.concatenate((np.ones(len(indices_for_fire)), np.zeros(len(undersampled_non_fire_indices))), axis=0)\n",
    "\n",
    "        #REPORT OUT \n",
    "        og_num_fire = sum(flat_labels)\n",
    "        og_num_no_fire = len(flat_labels) - sum(flat_labels)\n",
    "        print('You initially had %d fire observations and %d non-fire observations. You now have %d fire and %d non-fire.'\\\n",
    "             % (og_num_fire, og_num_no_fire, sum(Y_out), (len(Y_out)-sum(Y_out))))\n",
    "            \n",
    "        return X_out, Y_out\n",
    "\n",
    "Adventures in Saving Output Files\n",
    "\n",
    "    culled_inputs, culled_labels = cull(inputs, labels)\n",
    "    augd_culled_inputs = add_engineered(culled_inputs, nbrhood_radius=7) #So considering 15x15 grids centered at each pixel\n",
    "\n",
    "    X_eval_all_2d, Y_eval_all_1d = grids_to_pixels(augd_culled_inputs, culled_labels)\n",
    "\n",
    "    np.savetxt('X_eval_N7.txt', X_eval_all_2d)\n",
    "    np.savetxt('Y_eval_N7.txt', Y_eval_all_1d)\n",
    "\n",
    "Now for the evaluation data:\n",
    "\n",
    "    file_pattern = '/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_eval*'\n",
    "    side_length = 64 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
    "    num_obs = 164 #batch size\n",
    "\n",
    "    dataset = get_dataset(\n",
    "          file_pattern,\n",
    "          data_size=64,\n",
    "          sample_size=side_length,\n",
    "          batch_size=num_obs,\n",
    "          num_in_channels=12,\n",
    "          compression_type=None,\n",
    "          clip_and_normalize=False,\n",
    "          clip_and_rescale=False,\n",
    "          random_crop=True,\n",
    "          center_crop=False) \n",
    "\n",
    "    #Set up the data.\n",
    "    inputs, labels = next(iter(dataset)) #Easy to forget this line. If you do, you'll get suspiciously similar results to above!\n",
    "    culled_inputs, culled_labels = cull(inputs, labels)\n",
    "    augd_culled_inputs = add_engineered(culled_inputs, nbrhood_radius=2) #So considering 5x5 grids centered at each pixel\n",
    "    X_eval, Y_eval = grids_to_pixels(augd_culled_inputs, culled_labels)\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # Placeholder for the actual feature importance values\n",
    "    # In a real scenario, these would be computed based on the model's predictions\n",
    "    # For demonstration purposes, we'll generate some synthetic importance values\n",
    "    np.random.seed(0)  # For reproducibility\n",
    "    synthetic_importances = np.random.rand(13)\n",
    "\n",
    "    # Feature names, corresponding to your heatmap graphic\n",
    "    feature_names = [\n",
    "        'Elevation', 'Wind direction', 'Wind velocity', 'Min temp', 'Max temp',\n",
    "        'Humidity', 'Precip', 'Drought', 'Vegetation', 'Population density',\n",
    "        'Energy release component', 'Previous fire mask', 'Fire mask'\n",
    "    ]\n",
    "\n",
    "\\]\n",
    "\n",
    "    # Sort the features by their importance scores\n",
    "    sorted_indices = np.argsort(synthetic_importances)\n",
    "    sorted_features = np.array(feature_names)[sorted_indices]\n",
    "    sorted_importances = synthetic_importances[sorted_indices]\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(sorted_importances)), sorted_importances, color='skyblue', edgecolor='black')\n",
    "    plt.yticks(range(len(sorted_importances)), sorted_features)\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Feature Importance for Predicting Wildfire Spread')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/feature_importance.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    np.savetxt('X_eval_all.txt', X_eval)\n",
    "    np.savetxt('Y_eval_all.txt', Y_eval)\n",
    "\n",
    "    #Set up the data.\n",
    "    culled_inputs, culled_labels = cull(inputs, labels)\n",
    "    augd_culled_inputs = add_engineered(culled_inputs, nbrhood_radius=2) #So considering 5x5 grids centered at each pixel\n",
    "    X_train, Y_train = enact_undersample(augd_culled_inputs, culled_labels)\n",
    "\n",
    "    variables = ['elevation', 'wind speed', 'min temp', 'max temp', 'specific humidity', 'precipitation', 'drought index', \\\n",
    "                'vegetation index', 'population density', 'energy release component', 'previous mask', \\\n",
    "                 'average neigbors on fire', 'firewind score']\n",
    "\n",
    "    #Standardize the data, since linear regressions are sensitive to feature scale:\n",
    "    scaler = StandardScaler().fit(X_train) \n",
    "    X_train = scaler.transform(X_train) #confirm that sum of entries in X_train is MUCH smaller to sanity-check\n",
    "\n",
    "    You initially had 5873 fire observations and 342287 non-fire observations. You now have 5873 fire and 5873 non-fire.\n",
    "\n",
    "    # After culling and adding engineered features\n",
    "    print(\"Shapes after culling and engineering features:\")\n",
    "    print(\"Culled Inputs shape:\", culled_inputs.shape)\n",
    "    print(\"Culled Labels shape:\", culled_labels.shape)\n",
    "\n",
    "    # After undersampling\n",
    "    X_train, Y_train = enact_undersample(augd_culled_inputs, culled_labels)\n",
    "    print(\"Shapes after undersampling:\")\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"Y_train shape:\", Y_train.shape)\n",
    "\n",
    "    # Check the content of Y_train\n",
    "    print(\"Unique values in Y_train:\", np.unique(Y_train))\n",
    "\n",
    "    Shapes after culling and engineering features:\n",
    "    Culled Inputs shape: (85, 64, 64, 12)\n",
    "    Culled Labels shape: (85, 64, 64)\n",
    "    You initially had 5873 fire observations and 342287 non-fire observations. You now have 5873 fire and 5873 non-fire.\n",
    "    Shapes after undersampling:\n",
    "    X_train shape: (11746, 13)\n",
    "    Y_train shape: (11746,)\n",
    "    Unique values in Y_train: [0. 1.]\n",
    "\n",
    "    n_time_steps = 1  # Adjust based on your sequence length\n",
    "    n_features = X_train.shape[1]  # Number of features in your data\n",
    "\n",
    "    # Reshape to [samples, time_steps, features]\n",
    "    X_train_reshaped = X_train.reshape((X_train.shape[0], n_time_steps, n_features))\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "\n",
    "    # Split the data into a new training and validation set\n",
    "    X_train_new, X_val_new, Y_train_new, Y_val_new = train_test_split(\n",
    "        X_train_reshaped, Y_train, test_size=0.20, random_state=42\n",
    "    )\n",
    "\n",
    "    # Check the new shapes\n",
    "    print(\"New training data shape:\", X_train_new.shape)\n",
    "    print(\"New validation data shape:\", X_val_new.shape)\n",
    "\n",
    "    New training data shape: (9396, 1, 13)\n",
    "    New validation data shape: (2350, 1, 13)\n",
    "\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "    # Model definition\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adjusted LSTM layers\n",
    "    model.add(Bidirectional(LSTM(units=128, activation='tanh', kernel_regularizer=l2(0.001), return_sequences=True), input_shape=(n_time_steps, n_features)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(units=64, activation='tanh', kernel_regularizer=l2(0.001), return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(units=32, activation='tanh', kernel_regularizer=l2(0.001))))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Adjusted Dense layers\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(1, kernel_regularizer=l2(0.01)))\n",
    "\n",
    "    # Compile model with appropriate optimizer and loss function\n",
    "    optimizer = Adam(learning_rate=0.001)  # Adjusted learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=1e-6)\n",
    "\n",
    "    # Model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Training the model with modified parameters\n",
    "    history = model.fit(\n",
    "        X_train_new,\n",
    "        Y_train_new,\n",
    "        epochs=60,  # Adjusted based on early stopping\n",
    "        batch_size=64,  # Adjusted batch size\n",
    "        validation_data=(X_val_new, Y_val_new),\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "\n",
    "    # Visualize the model\n",
    "    plot_model(model, to_file='/kaggle/working/architecture.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\\[\\]\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')  # Add validation loss here\n",
    "    plt.title('Training & Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('/kaggle/working/training_validation_loss.png')  # Save the loss plot as a file\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the training and validation accuracy if it's available\n",
    "    if 'accuracy' in history.history and 'val_accuracy' in history.history:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')  # Add validation accuracy here\n",
    "        plt.title('Training & Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('/kaggle/working/training_validation_accuracy.png')  # Save the accuracy plot as a file\n",
    "    plt.show()\n",
    "\n",
    "\\[\\]\n",
    "\n",
    "\\[\\]\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # This code assumes 'history' is the output from your model's fit method\n",
    "    # You would replace 'loss' and 'val_loss' with the keys for your training and validation MSE\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training MSE')\n",
    "    plt.plot(history.history['val_loss'], label='Validation MSE')\n",
    "    plt.title('Model Mean Squared Error')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('/kaggle/working/MSE.png')  # Saves the plot as a PNG file\n",
    "    plt.show()\n",
    "\n",
    "Simple Linear Regression\n",
    "\n",
    "Still using all features (included the engineered ones), though.\n",
    "\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    #Set up the data.\n",
    "    culled_inputs, culled_labels = cull(inputs, labels)\n",
    "    augd_culled_inputs = add_engineered(culled_inputs, nbrhood_radius=2) #So considering 5x5 grids centered at each pixel\n",
    "    X_train, Y_train = enact_undersample(augd_culled_inputs, culled_labels)\n",
    "\n",
    "    variables = ['elevation', 'wind speed', 'min temp', 'max temp', 'specific humidity', 'precipitation', 'drought index', \\\n",
    "                'vegetation index', 'population density', 'energy release component', 'previous mask', \\\n",
    "                 'average neigbors on fire', 'firewind score']\n",
    "\n",
    "    #Standardize the data, since linear regressions are sensitive to feature scale:\n",
    "    scaler = StandardScaler().fit(X_train) \n",
    "    X_train = scaler.transform(X_train) #confirm that sum of entries in X_train is MUCH smaller to sanity-check\n",
    "\n",
    "    lr2 = LinearRegression().fit(X_train, Y_train)\n",
    "    print('Linear Regression Coefficients: ', lr2.coef_)\n",
    "\n",
    "    y_pred_raw = lr2.predict(X_train)\n",
    "\n",
    "    test_thresholds = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n",
    "    acc_scores = []  \n",
    "    TPR_scores = [] #TPR: true positive rate\n",
    "    TNR_scores = [] #TNR: true negative rate\n",
    "\n",
    "    for threshold in test_thresholds:\n",
    "        y_pred = [] #Need to reset for every threshold you try.\n",
    "        for i in range(len(y_pred_raw)):\n",
    "            if y_pred_raw[i] >= threshold:\n",
    "                y_pred.append(1)   \n",
    "            else:                                                          \n",
    "                y_pred.append(0)\n",
    "        #At this point we have the predictions for this value of the threshold:\n",
    "        y_pred = np.array(y_pred)\n",
    "        #Now we wish to analyze the prediction's preformance\n",
    "        CM = confusion_matrix(Y_train, y_pred) #Do NOT want to normalize this onesee below.\n",
    "        #Thanks, lucidv01d! https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "        FP = CM[0,1]\n",
    "        FN = CM[1,0]\n",
    "        TP = CM[1,1]\n",
    "        TN = CM[0,0]\n",
    "        acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "        TPR = TP/(TP+FN)\n",
    "        TNR = TN/(TN+FP)\n",
    "        #Record\n",
    "        acc_scores.append(acc)\n",
    "        TPR_scores.append(TPR)\n",
    "        TNR_scores.append(TNR)\n",
    "        \n",
    "    '''#And convert to numpy arrays to prepare to plot\n",
    "    y_pred = np.array(y_pred)\n",
    "    acc_scores = np.array(acc_scores)\n",
    "    '''\n",
    "    #Summarize performance with a plot:\n",
    "    plt.plot(test_thresholds, acc_scores, label = 'accuracy')\n",
    "    plt.plot(test_thresholds, TPR_scores, label = 'true positive rate')\n",
    "    plt.plot(test_thresholds, TNR_scores, label = 'true negative rate')\n",
    "    plt.xlabel('threshold choice')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title('Training Data Performance: Varying the Threshold')\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    my_thresh = 0.2\n",
    "\n",
    "    #Load validation data. We're NOT balancing its classes b/c the idea here is to estimate how the model would do on real data.\n",
    "    file_pattern = '/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_eval*'\n",
    "    side_length = 32 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
    "    num_obs = 100 #batch size\n",
    "\n",
    "    dataset = get_dataset(\n",
    "          file_pattern,\n",
    "          data_size=64,\n",
    "          sample_size=side_length,\n",
    "          batch_size=num_obs,\n",
    "          num_in_channels=12,\n",
    "          compression_type=None,\n",
    "          clip_and_normalize=False,\n",
    "          clip_and_rescale=False,\n",
    "          random_crop=True,\n",
    "          center_crop=False) \n",
    "\n",
    "    #Set up the data.\n",
    "    inputs, labels = next(iter(dataset)) #Easy to forget this line. If you do, you'll get suspiciously similar results to above!\n",
    "    culled_inputs, culled_labels = cull(inputs, labels)\n",
    "    augd_culled_inputs = add_engineered(culled_inputs, nbrhood_radius=2) #So considering 5x5 grids centered at each pixel\n",
    "    X_eval, Y_eval = grids_to_pixels(augd_culled_inputs, culled_labels)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_eval) \n",
    "    X_eval = scaler.transform(X_eval)\n",
    "\n",
    "    variables = ['elevation', 'wind speed', 'min temp', 'max temp', 'specific humidity', 'precipitation', 'drought index', \\\n",
    "                'vegetation index', 'population density', 'energy release component', 'previous mask', \\\n",
    "                 'average neigbors on fire', 'firewind score']\n",
    "\n",
    "    #Get the prediction\n",
    "    y_pred_raw = lr2.predict(X_eval)\n",
    "    y_pred = [] \n",
    "\n",
    "    for i in range(len(y_pred_raw)):\n",
    "        if y_pred_raw[i] >= my_thresh:\n",
    "            y_pred.append(1)   \n",
    "        else:                                                          \n",
    "            y_pred.append(0)\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    #Analyze the prediction's preformance\n",
    "    CM = confusion_matrix(Y_eval, y_pred) #Do NOT want to normalize this onesee below.\n",
    "    #Thanks, lucidv01d! https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "    FP = CM[0,1]\n",
    "    FN = CM[1,0]\n",
    "    TP = CM[1,1]\n",
    "    TN = CM[0,0]\n",
    "    acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    TPR = TP/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "\n",
    "    print('Accuracy: ', acc, 'TPR: ', TPR, 'TNR: ', TNR)\n",
    "    print('Proportion of labels that are on fire: ', sum(Y_eval)/len(Y_eval))\n",
    "\n",
    "    print('\\n LR coefficients:')\n",
    "    for i, coeff in enumerate(lr2.coef_):\n",
    "        print('{0:5s}  {1:>-10.2f}'.format(variables[i], np.round(coeff,2)))\n",
    "\n",
    "Linear Regression\n",
    "\n",
    "    my_thresh = 0.2\n",
    "\n",
    "    #Load the validation data.\n",
    "    file_pattern = '' \n",
    "      \n",
    "    side_length = 32 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
    "    num_obs = 100 #batch size\n",
    "\n",
    "    dataset = get_dataset(\n",
    "          file_pattern,\n",
    "          data_size=64,\n",
    "          sample_size=side_length,\n",
    "          batch_size=num_obs,\n",
    "          num_in_channels=12,\n",
    "          compression_type=None,\n",
    "          clip_and_normalize=False,\n",
    "          clip_and_rescale=False,\n",
    "          random_crop=True,\n",
    "          center_crop=False)\n",
    "\n",
    "    #Set up the data.\n",
    "    culled_inputs, culled_labels = cull(inputs, labels)\n",
    "    augd_culled_inputs = add_engineered(culled_inputs, nbrhood_radius=2) #So considering 5x5 grids centered at each pixel\n",
    "    X_eval, Y_eval = grids_to_pixels(augd_culled_inputs, culled_labels)\n",
    "    #Do NOT want to undersample with the validation data.\n",
    "\n",
    "    variables = ['elevation', 'wind speed', 'min temp', 'max temp', 'specific humidity', 'precipitation', 'drought index', \\\n",
    "                'vegetation index', 'population density', 'energy release component', 'previous mask', \\\n",
    "                 'average neigbors on fire', 'firewind score']\n",
    "\n",
    "    #Standardize the data, since linear regressions are sensitive to feature scale:\n",
    "    scaler = StandardScaler().fit(X_train) \n",
    "    X_train = scaler.transform(X_train) #confirm that sum of entries in X_train is MUCH smaller to sanity-check\n",
    "    X_eval = scaler.transform(X_eval) #Need to scale these too or the coefficients will be wrong. Checked with HW that \n",
    "                                       #both train and \"test\" should be transformed with the same scaler\n",
    "\n",
    "\n",
    "    #Next: Use the validation data to optimize LASSO regularization\n",
    "    from sklearn.linear_model import Lasso\n",
    "\n",
    "    n_fold = 10\n",
    "    n_feats = X_train.shape[1] #rows are observations, cols are features\n",
    "    alphas = np.logspace(-4, 4, num = 10) \n",
    "    coeffs = np.zeros((len(alphas), n_feats)) #rows correspond w/ choice for alpha, columns with feature coeffs.\n",
    "    #Loss function material: \n",
    "    TPR_scores = np.zeros(len(alphas)) \n",
    "    TNR_scores = np.zeros(len(alphas))\n",
    "\n",
    "\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        #Train a model, evaluate its performance, and record the result.\n",
    "        #Train\n",
    "        lasso = Lasso(alpha, random_state=2022, max_iter=50000).fit(X_train, Y_train)\n",
    "        \n",
    "        #Predict\n",
    "        y_pred_raw = lasso.predict(X_eval)\n",
    "        y_pred = [] \n",
    "        for j in range(len(y_pred_raw)): #An hour of debugging says: use a different index in the inner loop!\n",
    "            if y_pred_raw[j] >= my_thresh:\n",
    "                y_pred.append(1)   \n",
    "            else:                                                          \n",
    "                y_pred.append(0)\n",
    "\n",
    "        y_pred = np.array(y_pred) \n",
    "        \n",
    "        #Analyze the prediction's preformance\n",
    "        CM = confusion_matrix(Y_eval, y_pred) #Do NOT want to normalize this onesee below.\n",
    "        #Thanks, lucidv01d! https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "        FP = CM[0,1]\n",
    "        FN = CM[1,0]\n",
    "        TP = CM[1,1]\n",
    "        TN = CM[0,0]\n",
    "        acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "        TPR = TP/(TP+FN)\n",
    "        TNR = TN/(TN+FP)\n",
    "\n",
    "        #Record\n",
    "        coeffs[i,:] = lasso.coef_\n",
    "        TPR_scores[i] = TPR\n",
    "        TNR_scores[i] = TNR\n",
    "\n",
    "    fig, ax = plt.subplots(4,1,figsize=(15,16), sharex=True)\n",
    "\n",
    "    ax[0].plot(alphas, coeffs)\n",
    "    ax[0].set_xticks(alphas)\n",
    "    ax[0].set_ylabel('coefficient value')\n",
    "    ax[0].set_title('Lasso: Fire regression')\n",
    "\n",
    "    ax[1].plot(alphas, coeffs)\n",
    "    ax[1].set_ylabel('coefficient value')\n",
    "    ax[1].set_ylim([-0.01, 0.01])\n",
    "    ax[1].set_xlim([alphas[0], alphas[-1]])\n",
    "\n",
    "    ax[2].plot(alphas, TPR_scores, color='k')\n",
    "    ax[2].set_ylabel('True Positive Rate (validation)')\n",
    "    #ax[2].set_ylim([0.0,1.0])\n",
    "\n",
    "    ax[3].plot(alphas, TNR_scores, color='k')\n",
    "    ax[3].set_xscale('log')\n",
    "    ax[3].set_ylabel('True Negative Rate (validation)')\n",
    "    ax[3].set_xlabel('alpha')\n",
    "    #ax[3].set_ylim([0.7,0.95])\n",
    "\n",
    "\n",
    "    #POSSIBLY PLOT SPECIAL LOSS FUNCTION YOU DECIDE ON HERE\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "Choosing alpha = 0.005: Lets print out the stats.\n",
    "\n",
    "    my_alpha = 0.005\n",
    "    lasso = Lasso(my_alpha, random_state=2022, max_iter=50000).fit(X_train, Y_train)\n",
    "\n",
    "    y_pred_raw = lasso.predict(X_eval)\n",
    "    y_pred = [] \n",
    "    for j in range(len(y_pred_raw)): #An hour of debugging says: use a different index in the inner loop!\n",
    "        if y_pred_raw[j] >= my_thresh:\n",
    "            y_pred.append(1)   \n",
    "        else:                                                          \n",
    "            y_pred.append(0)\n",
    "\n",
    "    CM = confusion_matrix(Y_eval, y_pred) #Do NOT want to normalize this onesee below.\n",
    "    #Thanks, lucidv01d! https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "    FP = CM[0,1]\n",
    "    FN = CM[1,0]\n",
    "    TP = CM[1,1]\n",
    "    TN = CM[0,0]\n",
    "    acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    TPR = TP/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "\n",
    "    print('Accuracy: ', acc, 'TPR: ', TPR, 'TNR: ', TNR)\n",
    "    print('Proportion of labels that are on fire: ', sum(Y_eval)/len(Y_eval), '\\n')\n",
    "\n",
    "    print('LR coefficients:')\n",
    "    for i, coeff in enumerate(lasso.coef_):\n",
    "        print('{0:5s}  {1:>-10.2f}'.format(variables[i], np.round(coeff,2)))\n",
    "\n",
    "Decision Tree\n",
    "\n",
    "    #Set up the data. While the model outputs won't be any different if I leave X_train and X_eval scaled, the motivation for using\n",
    "    #a decision tree was to have a more transparent model, and working in real units will aid that transparency.\n",
    "\n",
    "    #TRAINING \n",
    "    file_pattern = '/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_train*' \n",
    "       \n",
    "    side_length = 32 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
    "    num_obs = 100 #batch size\n",
    "\n",
    "    dataset = get_dataset(\n",
    "          file_pattern,\n",
    "          data_size=64,\n",
    "          sample_size=side_length,\n",
    "          batch_size=num_obs,\n",
    "          num_in_channels=12,\n",
    "          compression_type=None,\n",
    "          clip_and_normalize=False,\n",
    "          clip_and_rescale=False,\n",
    "          random_crop=True,\n",
    "          center_crop=False)\n",
    "\n",
    "    #Set up the data.\n",
    "    culled_inputs, culled_labels = cull(inputs, labels)\n",
    "    augd_culled_inputs = add_engineered(culled_inputs, nbrhood_radius=2) #So considering 5x5 grids centered at each pixel\n",
    "    X_train, Y_train = grids_to_pixels(augd_culled_inputs, culled_labels)\n",
    "    X_train, Y_train = enact_undersample(augd_culled_inputs, culled_labels)\n",
    "\n",
    "    #VALIDATION\n",
    "    file_pattern = '/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_eval*' \n",
    "       \n",
    "    side_length = 32 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
    "    num_obs = 100 #batch size\n",
    "\n",
    "    dataset = get_dataset(\n",
    "          file_pattern,\n",
    "          data_size=64,\n",
    "          sample_size=side_length,\n",
    "          batch_size=num_obs,\n",
    "          num_in_channels=12,\n",
    "          compression_type=None,\n",
    "          clip_and_normalize=False,\n",
    "          clip_and_rescale=False,\n",
    "          random_crop=True,\n",
    "          center_crop=False)\n",
    "\n",
    "    #Set up the data.\n",
    "    culled_inputs, culled_labels = cull(inputs, labels)\n",
    "    augd_culled_inputs = add_engineered(culled_inputs, nbrhood_radius=2) #So considering 5x5 grids centered at each pixel\n",
    "    X_eval, Y_eval = grids_to_pixels(augd_culled_inputs, culled_labels)\n",
    "\n",
    "    #SAME FOR BOTH\n",
    "    variables = ['elevation', 'wind speed', 'min temp', 'max temp', 'specific humidity', 'precipitation', 'drought index', \\\n",
    "                'vegetation index', 'population density', 'energy release component', 'previous mask', \\\n",
    "                 'average neigbors on fire', 'firewind score']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #CHOOSING HYPERPARAMETERS: DEPTH \n",
    "    from sklearn.tree import DecisionTreeClassifier #don't copy and paste the regression version! :)\n",
    "\n",
    "    depths = np.arange(1,21,1) #[start, stop), step\n",
    "\n",
    "    TPR_scores = np.zeros(len(depths))\n",
    "    TNR_scores = np.zeros(len(depths))\n",
    "    acc_scores = np.zeros(len(depths))\n",
    "\n",
    "    for i, d in enumerate(depths):\n",
    "        #Train and predict.\n",
    "        tree_fire = DecisionTreeClassifier(max_depth=d, random_state=2022).fit(X_train, Y_train)\n",
    "        y_pred = tree_fire.predict(X_eval)\n",
    "        \n",
    "        #Evaluate\n",
    "        CM = confusion_matrix(Y_eval, y_pred) #Do NOT want to normalize this onesee below.\n",
    "        #Thanks, lucidv01d! https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "        FP = CM[0,1]\n",
    "        FN = CM[1,0]\n",
    "        TP = CM[1,1]\n",
    "        TN = CM[0,0]\n",
    "        acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "        TPR = TP/(TP+FN)\n",
    "        TNR = TN/(TN+FP)\n",
    "        \n",
    "        #Report out\n",
    "        TPR_scores[i] = TPR\n",
    "        TNR_scores[i] = TNR\n",
    "        acc_scores[i] = acc\n",
    "        \n",
    "        #/for loop\n",
    "\n",
    "    #Visualize results\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(depths, acc_scores, label = 'accuracy')\n",
    "    plt.plot(depths, TPR_scores, label = 'true positive rate')\n",
    "    plt.plot(depths, TNR_scores, label = 'true negative rate')\n",
    "\n",
    "    plt.xlabel('depth of tree')\n",
    "    plt.title('Decision Tree: Model Performance with Varying Depths')\n",
    "    plt.xticks(np.arange(0,22,2))\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "We now want to examine the best tree to see what its doing:\n",
    "\n",
    "    #First, make the best tree.\n",
    "    my_depth = 2\n",
    "    best_tree = DecisionTreeClassifier(max_depth=my_depth, random_state=2022).fit(X_train, Y_train)\n",
    "    y_pred = best_tree.predict(X_train)\n",
    "    y_eval_pred = best_tree.predict(X_eval)\n",
    "\n",
    "    #Print out its scores:\n",
    "\n",
    "    #Training\n",
    "    CM = confusion_matrix(Y_train, y_pred) #Do NOT want to normalize this onesee below.\n",
    "    #Thanks, lucidv01d! https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "    FP = CM[0,1]\n",
    "    FN = CM[1,0]\n",
    "    TP = CM[1,1]\n",
    "    TN = CM[0,0]\n",
    "    acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    TPR = TP/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "\n",
    "    print('Training accuracy: ', acc)\n",
    "    print('Training TPR: ', TPR)\n",
    "    print('Training TNR: ', TNR, '\\n\\n\\n')\n",
    "\n",
    "    #Validation\n",
    "    CM = confusion_matrix(Y_eval, y_eval_pred) #Do NOT want to normalize this onesee below.\n",
    "    #Thanks, lucidv01d! https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "    FP = CM[0,1]\n",
    "    FN = CM[1,0]\n",
    "    TP = CM[1,1]\n",
    "    TN = CM[0,0]\n",
    "    acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    TPR = TP/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "\n",
    "    print('Validation accuracy: ', acc) \n",
    "    print('Validation TPR: ', TPR)\n",
    "    print('Validation TNR: ', TNR, '\\n\\n\\n')\n",
    "\n",
    "    #See the feature importances:\n",
    "    print('Decision Tree Feature Importances:')\n",
    "    for i , var in enumerate(variables):\n",
    "      print('{0:5s}  {1:>-10.4f}'.format(var, np.round(best_tree.feature_importances_[i],4)))  \n",
    "\n",
    "    #Since the depth is small, it should be pretty easy to see what the tree is actually doing:\n",
    "    from sklearn import tree\n",
    "    fig = plt.figure(figsize=(30,16))\n",
    "    _ = tree.plot_tree(best_tree, feature_names=variables, class_names = ['no fire', 'fire'], filled=True, fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Can we improve on the single decision tree by using ensemble methods? To\n",
    "find out, first Ill try random forest and then Ill try adaptive\n",
    "boosting.\n",
    "\n",
    "    #Using the same reasoning as for the single decision tree above, I decided to use the real-units data for the random forest.\n",
    "    #I'm not using the OOB scores because I want to treat false positives and negatives separately, as I've been doing.\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    n_estimators = np.concatenate((np.arange(10, 100, 10), np.arange(100, 501, 100)))\n",
    "    acc_scores_train = np.zeros(len(n_estimators))\n",
    "    acc_scores_eval = np.zeros(len(n_estimators))\n",
    "\n",
    "    TPR_scores_train = np.zeros(len(n_estimators))\n",
    "    TPR_scores_eval = np.zeros(len(n_estimators))\n",
    "\n",
    "    TNR_scores_train = np.zeros(len(n_estimators))        \n",
    "    TNR_scores_eval = np.zeros(len(n_estimators))        \n",
    "\n",
    "    for i, n_est in enumerate(n_estimators):\n",
    "        rf = RandomForestClassifier(n_estimators=n_est, max_features=\"sqrt\", oob_score=False, random_state=2022)\n",
    "        rf.fit(X_train, Y_train)\n",
    "        \n",
    "        rf_train_pred = rf.predict(X_train)                \n",
    "        rf_eval_pred = rf.predict(X_eval)\n",
    "                                  \n",
    "        CM_train = confusion_matrix(Y_train, rf_train_pred)\n",
    "        FP = CM[0,1]\n",
    "        FN = CM[1,0]\n",
    "        TP = CM[1,1]\n",
    "        TN = CM[0,0]\n",
    "        acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "        TPR = TP/(TP+FN)\n",
    "        TNR = TN/(TN+FP)\n",
    "        \n",
    "        acc_scores_train[i] = acc                         \n",
    "        TPR_scores_train[i] = TPR\n",
    "        TNR_scores_train[i] = TNR\n",
    "                                  \n",
    "        CM_eval = confusion_matrix(Y_eval, rf_eval_pred)\n",
    "        FP = CM_eval[0,1]\n",
    "        FN = CM_eval[1,0]\n",
    "        TP = CM_eval[1,1]\n",
    "        TN = CM_eval[0,0]\n",
    "        acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "        TPR = TP/(TP+FN)\n",
    "        TNR = TN/(TN+FP)\n",
    "        \n",
    "        acc_scores_eval[i] = acc                         \n",
    "        TPR_scores_eval[i] = TPR\n",
    "        TNR_scores_eval[i] = TNR\n",
    "                                  \n",
    "\n",
    "    #And plot:\n",
    "    fig, ax = plt.subplots(2,1, sharex=True, figsize=(18,8))\n",
    "\n",
    "    #First figure: training performance\n",
    "    ax[0].set_title('Training Performance')                              \n",
    "    ax[0].plot(n_estimators, TPR_scores_train, color = 'g', label = 'true positive rate')\n",
    "    ax[0].plot(n_estimators, TNR_scores_train, color = 'b', label = 'true negative rate')\n",
    "    ax[0].plot(n_estimators, acc_scores_train, color = 'k', label = 'accuracy')  \n",
    "    ax[0].legend(loc = 'best')                              \n",
    "                                  \n",
    "    #Second figure: validation performance                              \n",
    "    ax[1].set_title('Validation Performance')                              \n",
    "    ax[1].plot(n_estimators, TPR_scores_eval, color = 'g', label = 'true positive rate')\n",
    "    ax[1].plot(n_estimators, TNR_scores_eval, color = 'b', label = 'true negative rate')\n",
    "    ax[1].plot(n_estimators, acc_scores_eval, color = 'k', label = 'accuracy')  \n",
    "    ax[1].legend(loc = 'best')                                 \n",
    "\n",
    "C Prepare the training and validation data\n",
    "\n",
    "    #Get the training data formatted:\n",
    "\n",
    "    #Clear away the observations with missing data\n",
    "    culled_inputs, culled_labels = cull(inputs, labels)\n",
    "\n",
    "    #New features\n",
    "    avg_nbr_feat = avg_neighbor_batch(culled_inputs[:, :, :, 11], city_radius = 2) #so: 5x5 grid ctrd at each pixel is considered\n",
    "    firewind_scores_feat = calc_firewind_scores(culled_inputs)\n",
    "\n",
    "    #Vectorize input data for training\n",
    "    #Supplied features\n",
    "    flat_elevation = vectorize(culled_inputs[:,:,:,0])\n",
    "    flat_wind_dir = vectorize(culled_inputs[:,:,:,1])\n",
    "    flat_wind_speed = vectorize(culled_inputs[:,:,:,2])\n",
    "    flat_min_temp = vectorize(culled_inputs[:,:,:,3])\n",
    "    flat_max_temp = vectorize(culled_inputs[:,:,:,4])\n",
    "    flat_humidity = vectorize(culled_inputs[:,:,:,5])\n",
    "    flat_precipitation = vectorize(culled_inputs[:,:,:,6])\n",
    "    flat_drought_index = vectorize(culled_inputs[:,:,:,7])\n",
    "    flat_vegetation = vectorize(culled_inputs[:,:,:,8])\n",
    "    flat_population_density = vectorize(culled_inputs[:,:,:,9])\n",
    "    flat_ERC = vectorize(culled_inputs[:,:,:,10])\n",
    "    flat_prev_masks = vectorize(culled_inputs[:,:,:,11])\n",
    "\n",
    "    #Derived features\n",
    "    flat_avg_nbrs = vectorize(avg_nbr_feat)\n",
    "    flat_firewind_scores = vectorize(firewind_scores_feat)\n",
    "\n",
    "    #Vectorize output data for training\n",
    "    flat_labels = vectorize(culled_labels)\n",
    "    Y_train = flat_labels\n",
    "\n",
    "    #Load the validation data\n",
    "\n",
    "\n",
    "    side_length = 32 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
    "    num_obs = 100 #batch size\n",
    "\n",
    "    dataset = get_dataset(\n",
    "          file_pattern,\n",
    "          data_size=64,\n",
    "          sample_size=side_length,\n",
    "          batch_size=num_obs,\n",
    "          num_in_channels=12,\n",
    "          compression_type=None,\n",
    "          clip_and_normalize=False,\n",
    "          clip_and_rescale=False,\n",
    "          random_crop=True,\n",
    "          center_crop=False)\n",
    "\n",
    "    inputs_eval, labels_eval = next(iter(dataset)) \n",
    "\n",
    "    #Toss out the observations with missing data\n",
    "    culled_inputs_eval, culled_labels_eval = cull(inputs_eval, labels_eval)\n",
    "\n",
    "    #New features\n",
    "    avg_nbr_feat_eval = avg_neighbor_batch(culled_inputs_eval[:, :, :, 11], city_radius = 2) #so: 5x5 grid ctrd at each pixel is considered\n",
    "    firewind_scores_feat_eval = calc_firewind_scores(culled_inputs_eval)\n",
    "\n",
    "    #Vectorize input data for training\n",
    "    #Supplied features\n",
    "    flat_elevation_eval = vectorize(culled_inputs_eval[:,:,:,0])\n",
    "    flat_wind_dir_eval = vectorize(culled_inputs_eval[:,:,:,1])\n",
    "    flat_wind_speed_eval = vectorize(culled_inputs_eval[:,:,:,2])\n",
    "    flat_min_temp_eval = vectorize(culled_inputs_eval[:,:,:,3])\n",
    "    flat_max_temp_eval = vectorize(culled_inputs_eval[:,:,:,4])\n",
    "    flat_humidity_eval = vectorize(culled_inputs_eval[:,:,:,5])\n",
    "    flat_precipitation_eval = vectorize(culled_inputs_eval[:,:,:,6])\n",
    "    flat_drought_index_eval = vectorize(culled_inputs_eval[:,:,:,7])\n",
    "    flat_vegetation_eval = vectorize(culled_inputs_eval[:,:,:,8])\n",
    "    flat_population_density_eval = vectorize(culled_inputs_eval[:,:,:,9])\n",
    "    flat_ERC_eval = vectorize(culled_inputs_eval[:,:,:,10])\n",
    "    flat_prev_masks_eval = vectorize(culled_inputs_eval[:,:,:,11])\n",
    "\n",
    "    #Derived features\n",
    "    flat_avg_nbrs_eval = vectorize(avg_nbr_feat_eval)\n",
    "    flat_firewind_scores_eval = vectorize(firewind_scores_feat_eval)\n",
    "\n",
    "    #Vectorize output data for evaluation data\n",
    "    flat_labels_eval = vectorize(culled_labels_eval)\n",
    "    Y_eval = flat_labels_eval\n",
    "\n",
    "\n",
    "    y_pred = []\n",
    "    threshold = 0.1 #set later, maybe with help of ROC curve or similar; for now just choose it.\n",
    "    for i in range(len(y_pred_raw)):\n",
    "        if y_pred_raw[i] >= threshold:\n",
    "            y_pred.append(1)   \n",
    "        else:                                                          \n",
    "            y_pred.append(0)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    CM = confusion_matrix(flat_labels, y_pred, normalize = 'all') #since I want to calculate\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(9,9))\n",
    "    disp = ConfusionMatrixDisplay(CM, display_labels=[0, 1]);\n",
    "    disp.plot(ax=ax)\n",
    "    ax.set_title('Confusion Matrix: Linear Classifier (Training Data)')\n",
    "    plt.show()\n",
    "    '''"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
